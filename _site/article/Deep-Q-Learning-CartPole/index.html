<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
    <style type="text/css">
	    .article-wrap img{border-radius: 5px;}
		.titled-image figure{text-align: center;}
	</style>
	<meta charset="utf-8">
<title>Vinit Sheth's Website  &#8211; Deep Q-Learning in OpenAI-Gym CartPole </title>
<meta name="description" content="">
<meta name="keywords" content="artificial intelligence, deep learning, reinforcement learning">




<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Q-Learning in OpenAI-Gym CartPole">
<meta property="og:description" content="Welcome to my site.">
<meta property="og:url" content="http://localhost:4000/article/Deep-Q-Learning-CartPole/">
<meta property="og:site_name" content="Vinit Sheth's Website">





<link rel="canonical" href="http://localhost:4000/article/Deep-Q-Learning-CartPole/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Vinit Sheth's Website Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/academicons.css" />

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">

<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">

<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="192x192" href="http://localhost:4000/images/apple-icon-precomposed.png">

</head>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000">Vinit Sheth's Website</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://localhost:4000/cv/" >Curriculum</a></li>
		        
				<li><a href="http://localhost:4000/projects/" >Projects</a></li>
		        
				<li><a href="http://localhost:4000/courses/" >Courses</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://localhost:4000/images//author_images/VinitSmall.jpg" class="bio-photo" alt="Vinit Sheth bio photo"></a>

<h3>Vinit Sheth</h3>
<p>Data Science , Machine Learning, Artificial Intelligence.</p>



<a href="http://linkedin.com/in/vinit-sheth" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vinitsheth" class="author-social" target="_blank"><i class="fa fa-github-square"></i> GitHub</a>









<a href="http://facebook.com/vinitgsheth" class="author-social" target="_blank"><i class="fa fa-facebook-square"></i> Facebook</a>
<a href="mailto:vsheth2@asu.edu" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>
<a href="https://drive.google.com/open?id=1ElcWvlcYgbx_Pw4amAY230AAM5meI6zQ" class="author-social" target="_blank"><i class="fa fa-file"></i>Resume</a>
  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://localhost:4000/article/Deep-Q-Learning-CartPole/" rel="bookmark" title="Deep Q-Learning in OpenAI-Gym CartPole">Deep Q-Learning in OpenAI-Gym CartPole</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <h3 id="introduction">Introduction</h3>

<p><a href="https://gym.openai.com/">OpenAI Gym</a> is a platform where you could test your intelligent learning algorithm in various application, including games and virtual physics experiments. It provides APIs for all these applications for the convenience of integrating the algorithms into the application. The API is called “environment” in OpenAI Gym. On one hand, the environment only receives “action” instructions as input and outputs the observation, reward, signal of termination, and other information. On the other hand, your learning algorithm receives observation(s), reward(s), signal(s) of termination as input and outputs the action. So in principle, one can develop a learning algorithm and wrapped it into a class object. It could test all the enviroments in OpenAI Gym.</p>

<p><br /></p>

<p>Because I have already implemented a Deep Q-Learning class to learn flappy bird, I think it would be very convenient to test the Deep Q-Learning algorithm in all these environments in OpenAI Gym.</p>

<p><img src="http://localhost:4000/images/articles/2017-04-28-Deep-Q-Learning-CartPole/cartpole.png" alt="" /></p>

<h3 id="make-openai-deep-q-learning-class">Make OpenAI Deep Q-Learning Class</h3>

<p>The environments in OpenAI Gym could be categorized into two classes regarding to their types of observation output. The video game environments usually outputs two-dimentional images as observation and the virtual physics experiments usually outputs one-dimentional numerical experiment observation data. Therefore, in addition to the existing Deep Q-Learning class for the two-dimentional image data, an additional Deep Q-Learning class that is suitable for learning from the one-dimentional data should be prepared for the OpenAI Gym environments.</p>

<h3 id="test-openai-deep-q-learning-class-in-openai-gym-cartpole-v0-environment">Test OpenAI Deep Q-Learning Class in OpenAI Gym CartPole-v0 Environment</h3>

<p>CartPole environment is probably the most simple environment in OpenAI Gym. However, when I was trying to load this environment, there is an issue regarding the box2d component. To fix this, please take the following steps. Many thanks to this <a href="http://kelisv.blogspot.com/2016/12/attributeerror-module-object-has-no.html">blogger</a> for the straightforward instruction. This bug might be fixed in the future release of OpenAI Gym according to someone related to OpenAI.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip uninstall Box2D box2d-py
git clone https://github.com/pybox2d/pybox2d
<span class="nb">cd </span>pybox2d/
python setup.py clean
python setup.py build
python setup.py install
</code></pre></div></div>

<p>At first I thought it would be super easy to train the Q-Learning algorithm, given a similar Q-Learning algorithm was doing extremely well in Flappy Bird game after training with 60,000 game frames. However, I was wrong in some aspects. With some parameter setting details from <a href="https://gym.openai.com/evaluations/eval_kBouPnRtQCezgE79s6aA5A">songrotek’s code</a>, I was able to overcome the problems and learned a lot. So I have to thank songrotek here.</p>

<h4 id="number-of-game-frames">Number of Game Frames</h4>

<p>When I was implementing Deep Q-Learning algorithm on Flappy Bird game, I used the concept of integrating multiple game frames as input data, because a single game frame is not able to fully represent the current state of the game. For example, the moving direction and moving velocity could not be told from a single game frame.</p>

<p><br /></p>

<p>There lacks the detailed explanations to the physical meannings to the actions and the observation in most of the environments in OpenAI Gym. (I already complained it in the forum, but it seems that there is nobody responding. ) This is also true for the CartPole-v0 environment. So I was not sure whether I have to ignore the observations preceeding to the current observation. In principle, I think including the proceeding observations will not hurt. Because even if the proceeding observations are not relavent to the determination of action, the neuron network will gave zero weights to those observations after sufficient training. However, it turned out that increasing game frames did not helped the algorithm learn well. If I set the game frame to 1, the algorithm was able to play CartPole very well after 5,000 to 8,000 episodes. If I set the game frame to 4, at least within 10,000 episodes, the algorithm was not able to play. I could set the episode maximum to 100,000 in the future to see whether a good learning performance could be achieved. But for this CartPole game, introducing multiple game frames is bad. If I knew the physical meaning of the observation data, I would not even try introducing multiple game frames. (It really made me sad when the algorithm did not work at the beginning.)</p>

<h4 id="neural-network">Neural Network</h4>

<p>Because the observation space of CartPole is only 3 and the action space of CartPole is only 2. I think this must be a very simple game. So I used one single layer of fully-connected neural network with only 20 hidden unit. It turned out that it worked just fine. It should be noted that there is no convolutional neural network in such applications.</p>

<h4 id="learning-rate">Learning Rate</h4>

<p>Learning rate is usually the most import parameter to the success of an algorithm in an application. Deep Learning is different to traditional Machine Learning. One may systematically explore all most all the hyperparameters in a Machine Learning task in a short period of time, however, the training of Deep Learning usually takes much longer time, which makes it much more difficult to tune deep learning hyperparameters using limited computation resources. In this situation, the experience, which I lack, becomes very important.</p>

<p><br /></p>

<p>In this CartPole game, I firstly set the learning rate to 0.0001 in Adam Optimizer, and started to observe the loss during the training. The loss increased right after the start of training, and the learning performance was extremely poor. So I thought the learning rate is too high. I immediately terminated the program and set the learning rate to smaller numbers. After training with smaller learning rates, say 0.000001, the loss decreasd after the start of training. But it stopped decreasing when the loss reaches around 0.4. The learning performance, in some rare cases, is extremely good. However, for the most of the time, the learning performance is extremely poor. I did not understand what’s happening at that time. Later, I think the optimization was trapped in local minimum at that time. The learning rate was too small for the optimization to overcome the barriers around the local minimum. That small learning rate in ordinary gradient descent leads to bad optimization outcome rarely happen in ordinary machine learning task to my knowledge, though it may take very long time to reach the minimum. I am not sure whether small learning rate sometimes would never lead the optimization to reach minimum if we use stochastic gradient descent, like what we used to use in Deep Learning tasks.</p>

<p><br /></p>

<p>It turned out that the learning rate of 0.0001 is the right one to use in CartPole game. The loss firstly increased then decreased. The algorithm was able to play CartPole very well after 5,000 to 8,000 episodes of training.</p>

<h3 id="key-parameters">Key Parameters</h3>

<p>FC-20</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GAME_STATE_FRAMES</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c"># number of game state frames used as input</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c"># decay rate of past observations</span>
<span class="n">EPSILON_INITIALIZED</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c"># probability epsilon used to determine random actions</span>
<span class="n">EPSILON_FINAL</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># final epsilon after decay</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span> <span class="c"># number of sample size in one minibatch</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0001</span> <span class="c"># learning rate in deep learning</span>
<span class="n">FRAME_PER_ACTION</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># number of frames per action</span>
<span class="n">REPLAYS_SIZE</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c"># maximum number of replays in cache</span>
<span class="n">TRAINING_DELAY</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c"># time steps before starting training for the purpose of collecting sufficient replays to initialize training</span>
<span class="n">EXPLORATION_TIME</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c"># time steps used for decaying epsilon during training before epsilon decreases to zero</span>
</code></pre></div></div>

<h3 id="algorithm-performance">Algorithm Performance</h3>

<p><strong>Before Training:</strong></p>

<p><img src="http://localhost:4000/images/articles/2017-04-28-Deep-Q-Learning-CartPole/episode_0.gif" alt="" /></p>

<p><strong>After Training:</strong></p>

<p><img src="http://localhost:4000/images/articles/2017-04-28-Deep-Q-Learning-CartPole/episode_27000.gif" alt="" /></p>

<p><strong>OpenAI Gym Evaluation</strong></p>

<p><br /></p>

<p>Solved after 9919 episodes. Best 100-episode average reward was 200.00 ± 0.00.
<a href="https://gym.openai.com/evaluations/eval_ewr0DWHeTmGE6x1NGQ1LiQ">https://gym.openai.com/evaluations/eval_ewr0DWHeTmGE6x1NGQ1LiQ</a></p>

<h3 id="conclusions">Conclusions</h3>

<p>Deep Q-Learning is a good technique to solve CartPole problem. However, it seems that it suffered from high variance and its convergences seems to be slow.</p>

<h3 id="links-to-github">Links to GitHub</h3>

<p><a href="https://github.com/leimao/OpenAI_Gym_AI/tree/master/CartPole-v0/Deep_Q-Learning/2017-04-28-v1">https://github.com/leimao/OpenAI_Gym_AI/tree/master/CartPole-v0/Deep_Q-Learning/2017-04-28-v1</a></p>

<h3 id="follow-up-optimizations">Follow-up Optimizations</h3>

<p>I used one single layer of fully-connected neural network with only 20 hidden unit in the first implementation. I found that increasing the depth and the size of neural network, and increasing the batch size for stochastic gradient descent could improve the learning efficiency and performance robustness. Personally I think the depth and the size of neural network helped to improve the robustness of performance, and the batch size helped to prevent random sampling bias and optimization bias during the stochastic gradient descent. As the result, the learning became faster, and the learning performance robustness was improved.</p>

<h4 id="2017-04-29-v1">2017-04-29-v1</h4>

<p><strong>Parameters</strong></p>

<p><br /></p>

<p>FC-128 -&gt; FC-128</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GAME_STATE_FRAMES</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c"># number of game state frames used as input</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="c"># decay rate of past observations</span>
<span class="n">EPSILON_INITIALIZED</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c"># probability epsilon used to determine random actions</span>
<span class="n">EPSILON_FINAL</span> <span class="o">=</span> <span class="mf">0.0001</span> <span class="c"># final epsilon after decay</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span> <span class="c"># number of sample size in one minibatch</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0005</span> <span class="c"># learning rate in deep learning</span>
<span class="n">FRAME_PER_ACTION</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># number of frames per action</span>
<span class="n">REPLAYS_SIZE</span> <span class="o">=</span> <span class="mi">2000</span> <span class="c"># maximum number of replays in cache</span>
<span class="n">TRAINING_DELAY</span> <span class="o">=</span> <span class="mi">2000</span> <span class="c"># time steps before starting training for the purpose of collecting sufficient replays to initialize training</span>
<span class="n">EXPLORATION_TIME</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c"># time steps used for decaying epsilon during training before epsilon decreases to zero</span>
</code></pre></div></div>

<p><strong>OpenAI Gym Evaluation</strong></p>

<p><br /></p>

<p>Solved after 293 episodes. Best 100-episode average reward was 197.39 ± 1.68.</p>

<p><br /></p>

<p><a href="https://gym.openai.com/evaluations/eval_Jr2oXkrS8KMUQEkCBurAw">https://gym.openai.com/evaluations/eval_Jr2oXkrS8KMUQEkCBurAw</a></p>

<p><br /></p>

<p><strong>Links to GitHub</strong></p>

<p><br /></p>

<p><a href="https://github.com/leimao/OpenAI_Gym_AI/tree/master/CartPole-v0/Deep_Q-Learning/2017-04-29-v1">https://github.com/leimao/OpenAI_Gym_AI/tree/master/CartPole-v0/Deep_Q-Learning/2017-04-29-v1</a></p>

<h4 id="2017-04-29-v2">2017-04-29-v2</h4>

<p><strong>Parameters</strong></p>

<p><br /></p>

<p>FC-128 -&gt; FC-128</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GAME_STATE_FRAMES</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c"># number of game state frames used as input</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="c"># decay rate of past observations</span>
<span class="n">EPSILON_INITIALIZED</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c"># probability epsilon used to determine random actions</span>
<span class="n">EPSILON_FINAL</span> <span class="o">=</span> <span class="mf">0.0005</span> <span class="c"># final epsilon after decay</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span> <span class="c"># number of sample size in one minibatch</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0005</span> <span class="c"># learning rate in deep learning</span>
<span class="n">FRAME_PER_ACTION</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># number of frames per action</span>
<span class="n">REPLAYS_SIZE</span> <span class="o">=</span> <span class="mi">5000</span> <span class="c"># maximum number of replays in cache</span>
<span class="n">TRAINING_DELAY</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c"># time steps before starting training for the purpose of collecting sufficient replays to initialize training</span>
<span class="n">EXPLORATION_TIME</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c"># time steps used for decaying epsilon during training before epsilon decreases to zero</span>
</code></pre></div></div>

<p><strong>OpenAI Gym Evaluation</strong></p>

<p><br /></p>

<p>Solved after 138 episodes. Best 100-episode average reward was 196.58 ± 1.34.</p>

<p><br /></p>

<p><a href="https://gym.openai.com/evaluations/eval_F90GxQxrQK2J6ESQkLVaA">https://gym.openai.com/evaluations/eval_F90GxQxrQK2J6ESQkLVaA</a></p>

<p><br /></p>

<p><strong>Links to GitHub</strong></p>

<p><br /></p>

<p><a href="https://github.com/leimao/OpenAI_Gym_AI/tree/master/CartPole-v0/Deep_Q-Learning/2017-04-29-v2">https://github.com/leimao/OpenAI_Gym_AI/tree/master/CartPole-v0/Deep_Q-Learning/2017-04-29-v2</a></p>

<h3 id="notes">Notes</h3>

<h4 id="2017-4-28">2017-4-28</h4>

<p>When I was training the algorithm, I found that if the algorithm was trained for sufficient long time, the learning performance would fluctuate. Say, the learning performance reached maximum at episode 5000 for 300 episodes. Then the learning performance dropped significantly. After training for some more time, the learning performance reached maximum again for another while. This phenomenon repeated throughout the training. From my personal point of view, the optimization might have deviated from the optimal because I could often see some large loss number even in the later stage of the training. Is it because the learning rate is sometimes to big to make cause the optimization jump out of the optimal, or it is often not possible to train an Deep Q-Learning algorithm to have an absolute perfect solution, or the neural network is just not sophiscated enough? I am not able to answer this question with my current knowledge.</p>

<p><br /></p>

<p>I was also suprised that if counting game frames, it also took nearly 1,000,000 game frames to reach good performance. Recall the a similar algorithm only took 600,000 game frames to have a extremely good performance in Flappy Bird game.</p>

<h4 id="2017-4-28-1">2017-4-28</h4>

<p>Specifically for the problem in OpenAI Gym, to achieve both learning efficiency and performance robustness, I think learning rate decay might be a good strategy. I may try it if I have chance in the future.</p>

<p><br /></p>

<p>I also found that, in addition to Q-Learning, Policy Gradient might work better. I may implement this algorithm in the future.</p>

<p><br /></p>

<p><a href="https://github.com/lancerts/Reinforcement-Learning">https://github.com/lancerts/Reinforcement-Learning</a></p>

<p><a href="https://gym.openai.com/evaluations/eval_9niu4HNZTgm0VLJ0b8MUtA">https://gym.openai.com/evaluations/eval_9niu4HNZTgm0VLJ0b8MUtA</a></p>

      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<img src="http://localhost:4000/images//author_images/VinitSmall.jpg" class="bio-photo" alt="Vinit Sheth bio photo"></a>

<h3>Vinit Sheth</h3>
<p>Data Science , Machine Learning, Artificial Intelligence.</p>



<a href="http://linkedin.com/in/vinit-sheth" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vinitsheth" class="author-social" target="_blank"><i class="fa fa-github-square"></i> GitHub</a>









<a href="http://facebook.com/vinitgsheth" class="author-social" target="_blank"><i class="fa fa-facebook-square"></i> Facebook</a>
<a href="mailto:vsheth2@asu.edu" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>
<a href="https://drive.google.com/open?id=1ElcWvlcYgbx_Pw4amAY230AAM5meI6zQ" class="author-social" target="_blank"><i class="fa fa-file"></i>Resume</a>
        </div>
       <!-- <p class="byline"><strong>Deep Q-Learning in OpenAI-Gym CartPole</strong> was published on <time datetime="2017-04-28T00:00:00-07:00">April 28, 2017</time> and last modified on <time datetime="2017-04-26">April 26, 2017</time> by <a href="http://localhost:4000" title="About Vinit Sheth">Vinit Sheth</a>.</p> -->
      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <footer>
    <span>&copy; 2019 Vinit Sheth. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<!-- Old
<script>
  var _gaq = _gaq || [];
  var pluginUrl = 
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-133931159-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
-->
<!-- New -->
<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-133931159-1', 'auto');
  ga('send', 'pageview');

</script>



  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'vinitsheth-github-io'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

	        

</body>
</html>