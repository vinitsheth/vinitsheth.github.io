<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
    <style type="text/css">
	    .article-wrap img{border-radius: 5px;}
		.titled-image figure{text-align: center;}
	</style>
	<meta charset="utf-8">
<title>Vinit Sheth's Website  &#8211; Maximum Likelihood Estimation of N-Gram Model Parameters </title>
<meta name="description" content="">
<meta name="keywords" content="Probability, Natural Language Processing">




<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Maximum Likelihood Estimation of N-Gram Model Parameters">
<meta property="og:description" content="Welcome to my site.">
<meta property="og:url" content="http://localhost:4000/blog/Maximum-Likelihood-Estimation-Ngram/">
<meta property="og:site_name" content="Vinit Sheth's Website">





<link rel="canonical" href="http://localhost:4000/blog/Maximum-Likelihood-Estimation-Ngram/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Vinit Sheth's Website Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='https://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/academicons.css" />

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">

<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">

<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="192x192" href="http://localhost:4000/images/apple-icon-precomposed.png">

</head>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000">Vinit Sheth's Website</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://localhost:4000/cv/" >Curriculum</a></li>
		        
				<li><a href="http://localhost:4000/projects/" >Projects</a></li>
		        
				<li><a href="http://localhost:4000/courses/" >Courses</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://localhost:4000/images//author_images/VinitSmall.jpg" class="bio-photo" alt="Vinit Sheth bio photo"></a>

<h3>Vinit Sheth</h3>
<p>Data Science , Machine Learning, Artificial Intelligence.</p>



<a href="http://linkedin.com/in/vinit-sheth" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vinitsheth" class="author-social" target="_blank"><i class="fa fa-github-square"></i> GitHub</a>









<a href="http://facebook.com/vinitgsheth" class="author-social" target="_blank"><i class="fa fa-facebook-square"></i> Facebook</a>
<a href="mailto:vsheth2@asu.edu" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>
<a href="https://drive.google.com/open?id=1ElcWvlcYgbx_Pw4amAY230AAM5meI6zQ" class="author-social" target="_blank"><i class="fa fa-file"></i>Resume</a>
  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://localhost:4000/blog/Maximum-Likelihood-Estimation-Ngram/" rel="bookmark" title="Maximum Likelihood Estimation of N-Gram Model Parameters">Maximum Likelihood Estimation of N-Gram Model Parameters</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <h3 id="introduction">Introduction</h3>

<p>A language model is a probability distribution over sequences of words, namely:</p>

<script type="math/tex; mode=display">p(w_1, w_2, w_3, ..., w_n)</script>

<p>According to the chain rule,</p>

<script type="math/tex; mode=display">p(w_1, w_2, w_3, ..., w_n) = p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)...p(w_n|w_{n-1},w_{n-2},...,w_1)</script>

<p>However, the parameters for this language model are $p(w_1)$, $p(w_2|w_1)$, …, $p(w_n|w_{n-1},…,w_1)$, which are usually too computationally expensive to calculate especially for the conditional probability with many conditioning words, even with a small dataset.</p>

<p><br /></p>

<p>To approximate $p(w_1, w_2, w_3, …, w_n)$, we could use N-Gram models to approximate the language model, namely:</p>

<h4 id="n-gram-model">N-Gram Model</h4>

<script type="math/tex; mode=display">p(w_1, w_2, w_3, ..., w_n) \approx p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)...p(w_n|w_{n-1},w_{n-2},...,w_{n-N})</script>

<p>In particular, we usually use unigram model, bigram model and trigram model in language modelings.</p>

<h4 id="unigram-model">Unigram Model</h4>

<script type="math/tex; mode=display">p(w_1, w_2, w_3, ..., w_n) \approx p(w_1)p(w_2)p(w_3)...p(w_n)</script>

<h4 id="bigram-model">Bigram Model</h4>

<script type="math/tex; mode=display">p(w_1, w_2, w_3, ..., w_n) \approx p(w_1)p(w_2|w_1)p(w_3|w_2)...p(w_n|w_{n-1})</script>

<h4 id="trigram-model">Trigram Model</h4>

<script type="math/tex; mode=display">p(w_1, w_2, w_3, ..., w_n) \approx p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)...p(w_n|w_{n-1},w_{n-2})</script>

<p>With the N-Gram model approximations, calculating $p(w_n|w_{n-1},w_{n-2},…,w_{n-N})$ is usually not too computationally expensive.</p>

<h3 id="maximum-likelihood-estimation-of-n-gram-model-parameters">Maximum Likelihood Estimation of N-Gram Model Parameters</h3>

<p>To estimate $p(w_n|w_{n-1},w_{n-2},…,w_{n-N})$, an intuitive way is to do  maximum likelihood estimation (MLE).</p>

<p><br /></p>

<p>Maximum likelihood esitmation estimates the model parameters such that the probability is maximized.</p>

<p><br /></p>

<p>In our case, the parameters are $p(w_n|w_{n-1},w_{n-2},…,w_{n-N})$, and the probability we maximizes is $p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)…p(w_n|w_{n-1},w_{n-2},…,w_{n-N})$</p>

<p><br /></p>

<p>In practice, we simply count the occurrance of word patterns to calculate the maximum likelihood estimation of $p(w_n|w_{n-1},w_{n-2},…,w_{n-N})$.</p>

<h4 id="unigram-model-1">Unigram Model</h4>

<script type="math/tex; mode=display">p(w_i) = \frac{c(w_i)}{\sum_{w}^{} c(w)}</script>

<h4 id="bigram-model-1">Bigram Model</h4>

<script type="math/tex; mode=display">p(w_i|w_{i-1}) = \frac{c(w_{i-1},w_i)}{\sum_{w}^{} c(w_{i-1},w)}</script>

<h4 id="trigram-model-1">Trigram Model</h4>

<script type="math/tex; mode=display">p(w_i|w_{i-1},w_{i-2}) = \frac{c(w_{i-2},w_{i-1},w_i)}{\sum_{w}^{} c(w_{i-2},w_{i-1},w)}</script>

<p>Now the question becomes why these formulas are the maximum likelihood estimations. Most of the books and online tutorials only gives these formulas without showing the formal mathematical proof.</p>

<p><br /></p>

<p>Here I am going to rigorously show that these are actually the formulas of maximum likelihood estimation.</p>

<h3 id="mathematical-derivation-of-maximum-likelihood-estimation-of-n-gram-model-parameters">Mathematical Derivation of Maximum Likelihood Estimation of N-Gram Model Parameters</h3>

<h4 id="unigram-model-2">Unigram Model</h4>

<p>Let us warm up with unigram model.</p>

<p><br /></p>

<p>We have a collection of unique words, $w_1, w_2, …, w_n$.</p>

<p><br /></p>

<p>For any given sequence of words $\mathbf{w}$ of length $N$ ($\mathbf{w} = (w_1, w_2, w_1, w_5, w_7, w_2)$ for example), we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(\mathbf{w}) 
& = p(w_1)^{c(w_1)} p(w_2)^{c(w_2)} p(w_3)^{c(w_3)}...p(w_n)^{c(w_n)}\\
& = \prod_{i=1}^{n}p(w_i)^{c(w_i)}
\end{aligned} %]]></script>

<p>where $c(w_i)$ is the count of word $w_i$ in the sentence.</p>

<p><br /></p>

<p>We take the log of $p(\mathbf{w})$, we then have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\log{p(\mathbf{w})}
& = c(w_1)\log{p(w_1)} + c(w_2)\log{p(w_2)} + c(w_3)\log{p(w_3)} + ... + c(w_n)\log{p(w_n)}\\
& = \sum_{i=1}^{n}c(w_i)\log{p(w_i)}
\end{aligned} %]]></script>

<p>To maximize $p(\mathbf{w})$, equivalently we have the following optimization problem:</p>

<p><br /></p>

<p>Maximize $\log{p(\mathbf{w})}$, subject to $\forall i \in [1 \dotsc N]$, $\sum_{i =  1}^{n} p(w_i) = 1$.</p>

<p><br /></p>

<p>Equivalently, we introduce auxilliary optimization function using Lagrange multiplier ($\sum_{i=1}^{n}p(w_i)-1 = 0$):</p>

<script type="math/tex; mode=display">\mathcal{L} = \sum_{i=1}^{n}c(w_i)\log{p(w_i)} + \lambda(\sum_{i=1}^{n}p(w_i)-1)</script>

<p>For any $p(w_j)$, we take the derivatives of $\mathcal{L}$ respective to $p(w_j)$:</p>

<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial p(w_j)} = \frac{c(w_j)}{p(w_j)} + \lambda = 0</script>

<script type="math/tex; mode=display">p(w_j) = -\frac{c(w_j)}{\lambda}</script>

<p>Because $\sum_{i=1}^{n}p(w_i) = 1$, we have:</p>

<script type="math/tex; mode=display">\sum_{i=1}^{n}p(w_i) = \sum_{i=1}^{n} -\frac{c(w_i)}{\lambda} = \frac{\sum_{i=1}^{n} c(w_i)}{-\lambda} = 1</script>

<script type="math/tex; mode=display">\lambda = - \sum_{i=1}^{n} c(w_i)</script>

<p>Because $p(w_j) = -c(w_j)/{\lambda}$, therefore</p>

<script type="math/tex; mode=display">p(w_j) = \frac{c(w_j)}{\sum_{i=1}^{n} c(w_i)}</script>

<p>This concludes the proof.</p>

<h4 id="bigram-model-2">Bigram Model</h4>

<p>Now let us move on to bigram model to see what is different.</p>

<p><br /></p>

<p>We have a collection of unique words, $w_1, w_2, …, w_n$.</p>

<p><br /></p>

<p>For the conditional probabilities, we have $n \times n$ possibilities.</p>

<p><br /></p>

<p>For any given sequence of words $\mathbf{w}$ of length $N$ ($\mathbf{w} = (w_1, w_2, w_1, w_5, w_7, w_2)$ for example), we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(\mathbf{w}) 
& = \prod_{i=1}^{n} p(w_i)^{s(w_i)} \prod_{i=1}^{n} \prod_{j=1}^{n} p(w_j|w_i)^{c(w_i, w_j)}
\end{aligned} %]]></script>

<p>where $c(w_i, w_j)$ is the count of word sequence $w_i, w_j$ in the sentence and</p>

<script type="math/tex; mode=display">% <![CDATA[
s(w_i) = \begin{cases}
    1, & \text{if $w_i$ is the first word}\\
    0, & \text{otherwise}
    \end{cases} %]]></script>

<p>We take the log of $p(\mathbf{w})$, we then have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\log{p(\mathbf{w})}
& = \sum_{i=1}^{n}s(w_i)\log{p(w_i)} + \sum_{i=1}^{n}\sum_{j=1}^{n}c(w_i,w_j)\log{p(w_j|w_i)}
\end{aligned} %]]></script>

<p>To maximize $p(\mathbf{w})$, equivalently we have the following optimization problem:</p>

<p><br /></p>

<p>Maximize $\log{p(\mathbf{w})}$, subject to $\forall i \in [1 \dotsc N]$, $\sum_{j =  1}^{n} p(w_j|w_i) = 1$.</p>

<p><br /></p>

<p>Equivalently, we introduce auxilliary optimization function using Lagrange multiplier ($\sum_{j =  1}^{n} p(w_j|w_i)-1 = 0$):</p>

<script type="math/tex; mode=display">\mathcal{L} = \sum_{i=1}^{n}s(w_i)\log{p(w_i)} + \sum_{i=1}^{n}\sum_{j=1}^{n}c(w_i,w_j)\log{p(w_j|w_i)} +  \sum_{i=1}^{n} \lambda_i \bigg( \big(\sum_{j = 1}^{n} p(w_j|w_i) \big) - 1 \bigg)</script>

<p>For any $p(w_k|w_i)$, we take the derivatives of $\mathcal{L}$ respective to $p(w_k|w_i)$:</p>

<script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial p(w_k|w_i)} = \frac{c(w_i, w_k)}{p(w_k|w_i)} + \lambda_i = 0</script>

<script type="math/tex; mode=display">p(w_k|w_i) = -\frac{c(w_i, w_k)}{\lambda_i}</script>

<p>Because $\sum_{j =  1}^{n} p(w_j|w_i) = 1$, we have:</p>

<script type="math/tex; mode=display">\sum_{j =  1}^{n} p(w_j|w_i) = \sum_{j =  1}^{n} -\frac{c(w_i, w_j)}{\lambda_i} = \frac{\sum_{j =  1}^{n} c(w_i, w_j)}{-\lambda_i} = 1</script>

<script type="math/tex; mode=display">\lambda_i = -\sum_{j =  1}^{n} c(w_i, w_j)</script>

<p>Because $p(w_k|w_i) = -c(w_i, w_k)/\lambda_i$, therefore</p>

<script type="math/tex; mode=display">p(w_k|w_i) = \frac{c(w_i, w_k)}{\sum_{j =  1}^{n} c(w_i, w_j)}</script>

<p>This concludes the proof.</p>

<h4 id="n-gram-model-1">N-Gram Model</h4>

<p>Without generality, the maximum likelihood estimation of n-gram model parameters could also be proved in the same way.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Mathematics is important for (statistical) machine learning.</p>

      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<img src="http://localhost:4000/images//author_images/VinitSmall.jpg" class="bio-photo" alt="Vinit Sheth bio photo"></a>

<h3>Vinit Sheth</h3>
<p>Data Science , Machine Learning, Artificial Intelligence.</p>



<a href="http://linkedin.com/in/vinit-sheth" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>


<a href="http://github.com/vinitsheth" class="author-social" target="_blank"><i class="fa fa-github-square"></i> GitHub</a>









<a href="http://facebook.com/vinitgsheth" class="author-social" target="_blank"><i class="fa fa-facebook-square"></i> Facebook</a>
<a href="mailto:vsheth2@asu.edu" class="author-social" target="_blank"><i class="fa fa-envelope-square"></i> E-Mail</a>
<a href="https://drive.google.com/open?id=1ElcWvlcYgbx_Pw4amAY230AAM5meI6zQ" class="author-social" target="_blank"><i class="fa fa-file"></i>Resume</a>
        </div>
       <!-- <p class="byline"><strong>Maximum Likelihood Estimation of N-Gram Model Parameters</strong> was published on <time datetime="2018-06-09T00:00:00-07:00">June 09, 2018</time> and last modified on <time datetime="2018-06-09">June 09, 2018</time> by <a href="http://localhost:4000" title="About Vinit Sheth">Vinit Sheth</a>.</p> -->
      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <footer>
    <span>&copy; 2019 Vinit Sheth. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


<!-- Asynchronous Google Analytics snippet -->
<!-- Old
<script>
  var _gaq = _gaq || [];
  var pluginUrl = 
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-133931159-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
-->
<!-- New -->
<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-133931159-1', 'auto');
  ga('send', 'pageview');

</script>



  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'vinitsheth-github-io'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

	        

</body>
</html>